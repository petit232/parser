import os
import json
import asyncio
import aiohttp
import time
import re
import socket
import geoip2.database
import logging
import base64
import subprocess
import shutil
from datetime import datetime
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

# ==============================================================================
# --- CONFIGURATION & LOGGING SETUP ---
# ==============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("MonsterEngine")

# --- DIRECTORY STRUCTURE ---
DB_DIR = 'database'   
OUTPUT_DIR = 'proxy'  

# Files inside "database"
SOURCE_FILE = 'all_sources.txt'
STATE_FILE = os.path.join(DB_DIR, 'monster_state.json')
GEOIP_DB = os.path.join(DB_DIR, 'GeoLite2-Country.mmdb')
LOCK_FILE = os.path.join(DB_DIR, '.monster.lock')

# Files inside "proxy"
LINKS_INFO_FILE = os.path.join(OUTPUT_DIR, 'LINKS_FOR_CLIENTS.txt')

# --- ENGINE CONSTANTS (MAX PERFORMANCE MODE) ---
TIMEOUT = 3              
MAX_CONCURRENCY = 1000   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ 20–∫+ —Å—Å—ã–ª–æ–∫
REST_INTERVAL_MIN = 10    # –í—Ä–µ–º—è –æ—Ç–¥—ã—Ö–∞ –º–µ–∂–¥—É –∫—Ä—É–≥–∞–º–∏ –ª–æ–≥–∏–∫–∏ (–º–∏–Ω—É—Ç—ã)
BATCH_SIZE = 100000       # –ë–µ—Ä–µ–º –∞–±—Å–æ–ª—é—Ç–Ω–æ –≤—Å–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –±–∞–∑—ã –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥

# --- NETWORK THRESHOLDS ---
PING_LIMITS = {
    'DEFAULT': 250,
    'US': 350, 
    'HK': 300, 
    'SG': 300, 
    'JP': 300,
    'BY': 200, 
    'KZ': 200, 
    'RU': 250
}

# –ü–æ—Ä–æ–≥–∏ –¥–ª—è "–∏–Ω–≤–∞–ª–∏–¥–Ω—ã—Ö" —Å–µ—Ä–≤–µ—Ä–æ–≤ (–º–µ–¥–ª–µ–Ω–Ω—ã–µ, –Ω–æ –∏–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤)
INVALID_THRESHOLD_MIN = 250
INVALID_THRESHOLD_MAX = 450 
INVALID_REGIONS = {'BY', 'KZ', 'US'}

# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –æ—á–µ—Ä–µ–¥–∏
PRIORITY_REGIONS = {'BY', 'KZ', 'DE', 'FI', 'SE', 'LV', 'RU', 'US', 'CH', 'FR'}

# --- COUNTRY TO FILE MAPPING (Strict 10 Countries) ---
COUNTRY_MAP = {
    'RU': 'russia.txt', 
    'BY': 'belarus.txt', 
    'DE': 'germany.txt',
    'FR': 'france.txt',
    'FI': 'finland.txt',
    'KZ': 'kazakhstan.txt',
    'LV': 'latvia.txt',
    'CH': 'switzerland.txt',
    'US': 'usa.txt',
    'SE': 'sweden.txt'
}
DEFAULT_MIX = 'mix.txt'
INVALID_FILE = 'invalid.txt' 
MAX_NODES_PER_FILE = 2500 # –õ–∏–º–∏—Ç –Ω–æ–¥ –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ –¥–ª—è –≤—ã–¥–∞—á–∏ –∫–ª–∏–µ–Ω—Ç–∞–º

class MonsterParser:
    """
    Advanced engine for parsing, checking, and distributing proxy configurations.
    Maintains strict file structure and ensures valid distribution by regions.
    """
    
    def __init__(self):
        self.start_time = time.time()
        self.ensure_structure()
        self.migrate_old_data()
        self.state = self.load_state()
        self.geo_reader = self.init_geo()
        
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.proxy_pattern = re.compile(r'(vless|vmess|trojan|ss|ssr)://[^\s"\'<>()]+')
        self.ip_pattern = re.compile(r'@?([\w\.-]+):(\d+)')
        
        self.semaphore = asyncio.Semaphore(MAX_CONCURRENCY)
        # 12 –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: 10 —Å—Ç—Ä–∞–Ω + –º–∏–∫—Å + –æ—Ç—Å—Ç–æ–π–Ω–∏–∫
        self.mandatory_files = set(COUNTRY_MAP.values()) | {DEFAULT_MIX, INVALID_FILE}

    def ensure_structure(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –ø–∞–ø–æ–∫ –∏ —Å–æ–∑–¥–∞–µ—Ç –∏—Ö —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
        if not os.path.exists(DB_DIR):
            os.makedirs(DB_DIR, exist_ok=True)
        if not os.path.exists(OUTPUT_DIR):
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            
        if not os.path.exists(SOURCE_FILE):
            with open(SOURCE_FILE, 'w', encoding='utf-8') as f:
                f.write("# Monster Engine Source List\n")

    def migrate_old_data(self):
        """–ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö –ø–∞–ø–æ–∫ –≤ –ª–∞—Ç–∏–Ω–∏—Ü—É (—Ç–µ —Å–∞–º—ã–µ 15 —Å—Ç—Ä–æ–∫ –º–∏–≥—Ä–∞—Ü–∏–∏)."""
        old_folders = {'–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö': DB_DIR, '–ø—Ä–æ–∫—Å–∏': OUTPUT_DIR}
        for old, new in old_folders.items():
            if os.path.exists(old) and os.path.isdir(old):
                logger.info(f"üîÑ Migration: '{old}' -> '{new}'")
                for item in os.listdir(old):
                    src = os.path.join(old, item)
                    dst = os.path.join(new, item)
                    try:
                        if os.path.exists(dst): 
                            os.remove(src)
                        else: 
                            shutil.move(src, dst)
                    except Exception as e:
                        logger.debug(f"Migration error: {e}")
                try: 
                    os.rmdir(old)
                except: 
                    pass

    def init_geo(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã GeoIP2 —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –±–∏—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤."""
        if not os.path.exists(GEOIP_DB):
            logger.warning(f"‚ö†Ô∏è GeoIP Database NOT FOUND at {GEOIP_DB}. Sorting will be limited.")
            return None
            
        try:
            file_size = os.path.getsize(GEOIP_DB)
            if file_size < 1048576: 
                logger.error(f"‚ùå GeoIP file is too small ({file_size} bytes). Likely corrupted or 404. Skipping.")
                return None

            reader = geoip2.database.Reader(GEOIP_DB)
            reader.country('8.8.8.8') 
            logger.info(f"‚úÖ GeoIP Engine ready (Size: {file_size/1024/1024:.2f} MB)")
            return reader
        except Exception as e:
            logger.error(f"‚ùå GeoIP Init Error: {e}")
            try:
                if "valid MaxMind" in str(e) or "not a valid" in str(e).lower():
                    os.remove(GEOIP_DB)
                    logger.info("üóëÔ∏è Corrupted GeoIP file removed for re-download.")
            except: pass
            return None

    def load_state(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞."""
        default = {"last_index": 0, "processed_total": 0, "dead_total": 0}
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return {**default, **data}
            except: pass
        return default

    def save_state(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞."""
        try:
            with open(STATE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=4, ensure_ascii=False)
        except Exception as e:
            logger.error(f"Save State Error: {e}")

    def get_host_port(self, link):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–æ—Å—Ç–∞ –∏ –ø–æ—Ä—Ç–∞ –∏–∑ —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–µ—Ç–µ–≤–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏."""
        try:
            match = self.ip_pattern.search(link)
            if match: return match.group(1), match.group(2)
        except: pass
        return None, None

    async def fetch_subscription(self, session, url):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Å—ã–ª–∫–µ-–ø–æ–¥–ø–∏—Å–∫–µ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ Base64)."""
        try:
            async with session.get(url, timeout=20) as response:
                if response.status == 200:
                    text = await response.text()
                    try:
                        decoded = base64.b64decode(text.strip()).decode('utf-8', errors='ignore')
                        if '://' in decoded: text = decoded
                    except: pass
                    return [m.group(0) for m in self.proxy_pattern.finditer(text)]
        except: pass
        return []

    async def check_node(self, session, host, port, ip_cache):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–¥—ã: DNS —Ä–µ–∑–æ–ª–≤ –∏ TCP –∫–æ–Ω–Ω–µ–∫—Ç —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Å–µ–º–∞—Ñ–æ—Ä–æ–º."""
        if not host or not port: return None, 9999
        
        key = f"{host}:{port}"
        if key in ip_cache: return ip_cache[key]
        
        async with self.semaphore:
            start = time.time()
            try:
                # DNS RESOLVE
                ip = await asyncio.wait_for(
                    asyncio.get_event_loop().run_in_executor(None, socket.gethostbyname, host),
                    timeout=TIMEOUT
                )
                # TCP CONNECT
                _, writer = await asyncio.wait_for(
                    asyncio.open_connection(ip, int(port)),
                    timeout=TIMEOUT
                )
                writer.close()
                await writer.wait_closed()
                
                latency = int((time.time() - start) * 1000)
                res = (ip, latency)
                ip_cache[key] = res
                return res
            except:
                ip_cache[key] = (None, 9999)
                return None, 9999

    def get_country_code(self, ip):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –ø–æ IP —á–µ—Ä–µ–∑ GeoIP2 –±–∞–∑—É."""
        if not self.geo_reader or not ip: return None
        try:
            return self.geo_reader.country(ip).country.iso_code
        except: return None

    def apply_fragmentation(self, link):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è VLESS/VMESS/Trojan (–æ–±—Ö–æ–¥ –¢–°–ü–£ –†–§)."""
        try:
            parsed = urlparse(link)
            if parsed.scheme in ['vless', 'vmess', 'trojan']:
                query = parse_qs(parsed.query)
                if 'reality' in str(query).lower(): return link
                query['fragment'] = ['10-20,30-50']
                query['mux'] = ['enable=true&concurrency=8']
                parts = list(parsed)
                parts[4] = urlencode(query, doseq=True)
                return urlunparse(parts)
        except: pass
        return link

    def update_links_manifest(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ —Å—Å—ã–ª–æ–∫ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ GitHub Raw)."""
        try:
            base_url = "https://raw.githubusercontent.com/USER/REPO/main/proxy"
            try:
                remote = subprocess.check_output(["git", "config", "--get", "remote.origin.url"]).decode().strip()
                path = remote.replace("git@github.com:", "").replace("https://github.com/", "").replace(".git", "")
                base_url = f"https://raw.githubusercontent.com/{path}/main/{OUTPUT_DIR}"
            except: pass

            lines = [
                "üöÄ MONSTER ENGINE - LIVE PROXY LINKS",
                "="*40,
                f"Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                "Note: Automatic high-speed distribution is active.",
                ""
            ]
            
            for file in sorted(list(self.mandatory_files)):
                name = file.replace('.txt', '').upper()
                lines.append(f"üìç {name}:")
                lines.append(f"{base_url}/{file}")
                lines.append("")
                
            with open(LINKS_INFO_FILE, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines))
        except Exception as e:
            logger.error(f"Manifest Error: {e}")

    async def execute_cycle(self):
        """–û–¥–∏–Ω–æ—á–Ω—ã–π –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞–∑—ã."""
        if os.path.exists(LOCK_FILE):
            if (time.time() - os.path.getmtime(LOCK_FILE)) < 300:
                logger.warning("Engine cycle skipped: lock file is too fresh.")
                return

        try:
            with open(LOCK_FILE, 'w') as f: f.write(str(time.time()))
            
            if not os.path.exists(SOURCE_FILE): return
            with open(SOURCE_FILE, 'r', encoding='utf-8', errors='ignore') as f:
                raw = [l.strip() for l in f if len(l.strip()) > 10 and not l.startswith('#')]
            
            subs = [u for u in raw if u.startswith('http')]
            configs = [c for c in raw if '://' in c and not c.startswith('http')]
            
            all_links = configs.copy()
            async with aiohttp.ClientSession() as session:
                logger.info(f"üì• Fetching {len(subs)} subscriptions...")
                sub_data = await asyncio.gather(*[self.fetch_subscription(session, u) for u in subs])
                for links in sub_data: all_links.extend(links)
                
                unique_pool = set(all_links)
                self.update_links_manifest()
                
                total = len(unique_pool)
                logger.info(f"üîé Total unique nodes discovered: {total}")
                
                if total == 0:
                    for filename in self.mandatory_files:
                        p = os.path.join(OUTPUT_DIR, filename)
                        with open(p, 'w', encoding='utf-8') as f: f.write('')
                    return

                # –ü–†–ò–û–†–ò–¢–ï–¢–ù–ê–Ø –°–û–†–¢–ò–†–û–í–ö–ê
                pool_list = list(unique_pool)
                pool_list.sort(key=lambda x: any(reg in x.upper() for reg in PRIORITY_REGIONS), reverse=True)
                
                # –†–ï–ñ–ò–ú –ú–û–ù–°–¢–†–ê: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å—ë —Å—Ä–∞–∑—É
                batch = pool_list[:BATCH_SIZE]
                logger.info(f"‚ö° Checking {len(batch)} nodes with concurrency {MAX_CONCURRENCY}...")
                
                ip_cache = {}
                results = await asyncio.gather(*[self.check_node(session, *self.get_host_port(l), ip_cache) for l in batch])
                
                valid_nodes = []
                dead_set = set()
                
                for i, (ip, ping) in enumerate(results):
                    link = batch[i]
                    if not ip:
                        dead_set.add(link)
                        continue
                        
                    cc = self.get_country_code(ip)
                    limit = PING_LIMITS.get(cc, PING_LIMITS['DEFAULT'])
                    
                    if ping <= limit:
                        final_link = self.apply_fragmentation(link) if cc == 'RU' else link
                        valid_nodes.append({"link": final_link, "cc": cc, "type": "good"})
                    elif cc in INVALID_REGIONS and INVALID_THRESHOLD_MIN <= ping <= INVALID_THRESHOLD_MAX:
                        valid_nodes.append({"link": link, "cc": cc, "type": "invalid"})
                    else:
                        dead_set.add(link)

            # --- –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –§–ê–ô–õ–ê–ú ---
            for filename in self.mandatory_files:
                target_path = os.path.join(OUTPUT_DIR, filename)
                file_content = {}
                fallback_candidate = None

                # 1. –ß–∏—Ç–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ (—Å–ø–∞—Å–∞–µ–º "–≤—ã–∂–∏–≤—à–∏—Ö")
                if os.path.exists(target_path):
                    try:
                        with open(target_path, 'r', encoding='utf-8') as f:
                            for line in f:
                                node = line.strip()
                                if "://" in node:
                                    if not fallback_candidate and node not in dead_set:
                                        fallback_candidate = node
                                    if node in unique_pool and node not in dead_set:
                                        file_content[node] = True
                    except: pass
                
                # 2. –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –≤–∞–ª–∏–¥–Ω—ã–µ –Ω–æ–¥—ã
                for node_data in valid_nodes:
                    node_cc, node_link = node_data['cc'], node_data['link']
                    if node_data['type'] == "invalid" and filename == INVALID_FILE:
                        file_content[node_link] = True
                    elif node_data['type'] == "good":
                        if COUNTRY_MAP.get(node_cc, DEFAULT_MIX) == filename:
                            file_content[node_link] = True

                # 3. –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ (–Ω–µ –±–æ–ª–µ–µ MAX_NODES_PER_FILE)
                final_list = list(file_content.keys())[:MAX_NODES_PER_FILE]
                
                # –ó–ê–©–ò–¢–ê: –ï—Å–ª–∏ —Ñ–∞–π–ª –ø—É—Å—Ç, –Ω–æ —É –Ω–∞—Å –µ—Å—Ç—å —Ö–æ—Ç—å –æ–¥–Ω–∞ –∂–∏–≤–∞—è –Ω–æ–¥–∞ - –≤—Ç—ã–∫–∞–µ–º –µ—ë –∫–∞–∫ —â–∏—Ç
                if not final_list and fallback_candidate:
                    final_list = [fallback_candidate]

                with open(target_path, 'w', encoding='utf-8') as f:
                    if final_list:
                        f.write('\n'.join(final_list) + '\n')
                    else:
                        f.write(f"# Monster Update: {datetime.now().strftime('%H:%M:%S')} | No active nodes\n")
                
                os.utime(target_path, None)

            # –û—á–∏—Å—Ç–∫–∞ all_sources.txt –æ—Ç –º–µ—Ä—Ç–≤–µ—Ü–æ–≤ (–∫—Ä–æ–º–µ –ø–æ–¥–ø–∏—Å–æ–∫)
            updated_sources = [s for s in raw if s.startswith('http') or (s in unique_pool and s not in dead_set)]
            with open(SOURCE_FILE, 'w', encoding='utf-8') as f:
                f.write('\n'.join(updated_sources) + '\n')

            self.state.update({
                "processed_total": self.state.get("processed_total", 0) + len(batch),
                "last_run": datetime.now().isoformat()
            })
            self.save_state()
            logger.info("‚úÖ Cycle finished successfully.")

        except Exception as e:
            logger.error(f"üö® Engine Error: {e}", exc_info=True)
        finally:
            if os.path.exists(LOCK_FILE):
                try: os.remove(LOCK_FILE)
                except: pass

    async def main_loop(self):
        """–ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ä–µ–∂–∏–º: —Ü–∏–∫–ª -> –æ—Ç–¥—ã—Ö -> —Ü–∏–∫–ª."""
        while True:
            logger.info("üåÄ Starting new Monster Cycle...")
            await self.execute_cycle()
            logger.info(f"üí§ Sleeping for {REST_INTERVAL_MIN} minutes before next run...")
            await asyncio.sleep(REST_INTERVAL_MIN * 60)

if __name__ == "__main__":
    # –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞: –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –∏–ª–∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π –¥–µ–º–æ–Ω
    engine = MonsterParser()
    try:
        # –î–ª—è GitHub Actions –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å engine.execute_cycle()
        # –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞/VPS –ª—É—á—à–µ engine.main_loop()
        asyncio.run(engine.execute_cycle()) 
    except KeyboardInterrupt:
        logger.info("Engine stopped by user.")
