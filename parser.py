import os
import json
import asyncio
import aiohttp
import time
import re
import socket
import geoip2.database
import logging
from datetime import datetime
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

# --- LOGGING SETUP ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("MonsterEngine")

# --- CONFIGURATION ---
SOURCE_FILE = 'all_sources.txt'
STATE_FILE = 'monster_state.json'
GEOIP_DB = 'GeoLite2-Country.mmdb'
LINKS_INFO_FILE = 'LINKS_FOR_CLIENTS.txt'
LOCK_FILE = '.monster.lock'

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
TIMEOUT = 3            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ (DNS + TCP)
MAX_CONCURRENCY = 150  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
CYCLE_HOURS = 3        # –ó–∞ —Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –≤—Å—é –±–∞–∑—É –ø–æ–ª–Ω–æ—Å—Ç—å—é
BATCH_INTERVAL_MIN = 20 # –ö–∞–∫ —á–∞—Å—Ç–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —Å–∫—Ä–∏–ø—Ç (—á–µ—Ä–µ–∑ GitHub Actions cron)

# –ü–æ—Ä–æ–≥–∏ –ø–∏–Ω–≥–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö)
PING_LIMITS = {
    'DEFAULT': 200,
    'US': 220, 'HK': 220, 'SG': 220, 'JP': 220,
    'BY': 200, 'KZ': 200, 'RU': 200
}

# –†–µ–≥–∏–æ–Ω—ã —Å –Ω–∞–∏–≤—ã—Å—à–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º (–ë–µ–ª–∞—Ä—É—Å—å, –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, –ï–≤—Ä–æ–ø–∞)
PRIORITY_REGIONS = {'BY', 'KZ', 'PL', 'DE', 'FI', 'SE', 'LT', 'LV', 'EE', 'RU'}

COUNTRY_MAP = {
    'RU': 'russia.txt', 'BY': 'belarus.txt', 'FI': 'finland.txt',
    'FR': 'france.txt', 'DE': 'germany.txt', 'HK': 'hongkong.txt',
    'KZ': 'kazakhstan.txt', 'NL': 'netherlands.txt', 'PL': 'poland.txt',
    'SG': 'singapore.txt', 'SE': 'sweden.txt', 'GB': 'uk.txt', 'US': 'usa.txt',
}
DEFAULT_MIX = 'mix.txt'
MAX_NODES_PER_COUNTRY = 500

class MonsterParser:
    def __init__(self):
        self.state = self.load_state()
        self.geo_reader = None
        try:
            if os.path.exists(GEOIP_DB):
                self.geo_reader = geoip2.database.Reader(GEOIP_DB)
        except Exception as e:
            logger.error(f"GeoIP Database error: {e}")
        
        # –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–æ—Å—Ç–∞ –∏ –ø–æ—Ä—Ç–∞
        self.ip_pattern = re.compile(r'@?([\w\.-]+):(\d+)')
        self.semaphore = asyncio.Semaphore(MAX_CONCURRENCY)

    def load_state(self):
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE, 'r') as f:
                    return json.load(f)
            except Exception: pass
        return {"last_index": 0, "processed_total": 0, "history": []}

    def save_state(self):
        try:
            with open(STATE_FILE, 'w') as f:
                json.dump(self.state, f, indent=4)
        except Exception: pass

    def get_host_port(self, link):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–æ—Å—Ç –∏ –ø–æ—Ä—Ç –∏–∑ –ø—Ä–æ–∫—Å–∏-—Å—Å—ã–ª–∫–∏"""
        try:
            match = self.ip_pattern.search(link)
            if match:
                return match.group(1), match.group(2)
        except Exception: pass
        return None, None

    async def check_node(self, session, host, port, ip_cache):
        """
        –£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π TCP-–ø–∏–Ω–≥ —Å –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–º DNS.
        """
        cache_key = f"{host}:{port}"
        if cache_key in ip_cache:
            return ip_cache[cache_key]
        
        async with self.semaphore:
            start_time = time.time()
            try:
                # DNS –†–µ–∑–æ–ª–≤–∏–Ω–≥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ (—á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å asyncio)
                ip_addr = await asyncio.wait_for(
                    asyncio.get_event_loop().run_in_executor(None, socket.gethostbyname, host),
                    timeout=TIMEOUT
                )
                
                # –ü–æ–ø—ã—Ç–∫–∞ TCP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                target_port = int(port) if port else 443
                conn = asyncio.open_connection(ip_addr, target_port)
                reader, writer = await asyncio.wait_for(conn, timeout=TIMEOUT)
                
                writer.close()
                await writer.wait_closed()
                
                ping_ms = int((time.time() - start_time) * 1000)
                res = (ip_addr, ping_ms)
                ip_cache[cache_key] = res
                return res
            except Exception:
                ip_cache[cache_key] = (None, 9999)
                return None, 9999

    def get_country(self, ip):
        if not self.geo_reader or not ip: return None
        try:
            return self.geo_reader.country(ip).country.iso_code
        except Exception: return None

    def wrap_for_russia(self, link):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ DPI –≤ –†–§"""
        try:
            parsed = urlparse(link)
            if not parsed.scheme or (not parsed.netloc and '@' not in link): return link
            query = parse_qs(parsed.query)
            
            # –ù–µ —Ç—Ä–æ–≥–∞–µ–º Reality, –æ–Ω–∏ –∏ —Ç–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç
            if 'reality' in str(query.get('security', [])).lower(): return link
            
            if parsed.scheme in ['vless', 'vmess', 'trojan']:
                query['fragment'] = ['10-20,30-50']
                query['mux'] = ['enable=true&concurrency=8']
                if 'security' not in query: query['security'] = ['tls']
                
                new_parts = list(parsed)
                new_parts[4] = urlencode(query, doseq=True)
                return urlunparse(new_parts)
            return link
        except Exception: return link

    async def run(self):
        # 1. –ó–∞—â–∏—Ç–∞ –æ—Ç –Ω–∞—Å–ª–æ–µ–Ω–∏—è (Lock File)
        if os.path.exists(LOCK_FILE):
            lock_age = time.time() - os.path.getmtime(LOCK_FILE)
            if lock_age < 1200: # 20 –º–∏–Ω—É—Ç –ª–∏–º–∏—Ç –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω
                logger.warning(f"Process already running (Age: {int(lock_age)}s). Aborting.")
                return
            else:
                os.remove(LOCK_FILE)
        
        try:
            with open(LOCK_FILE, 'w') as f: f.write(str(time.time()))

            if not os.path.exists(SOURCE_FILE):
                logger.error("Source file missing!")
                return

            # –ß–∏—Ç–∞–µ–º –∏ —á–∏—Å—Ç–∏–º –±–∞–∑—É –æ—Ç –¥—É–±–ª–µ–π
            with open(SOURCE_FILE, 'r', encoding='utf-8', errors='ignore') as f:
                links = [l.strip() for l in f if len(l.strip()) > 10]
            
            links = list(dict.fromkeys(links))
            total_count = len(links)
            
            # 2. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ (3-—á–∞—Å–æ–≤–æ–π —Ü–∏–∫–ª)
            runs_per_cycle = (CYCLE_HOURS * 60) / BATCH_INTERVAL_MIN
            batch_size = max(500, int(total_count / runs_per_cycle))
            
            # 3. –£–º–Ω–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º BY, KZ –∏ —Ç–µ, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã –≤ —Å—Å—ã–ª–∫–µ
            links.sort(key=lambda x: any(p in x.upper() for p in PRIORITY_REGIONS), reverse=True)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞
            start_idx = self.state.get("last_index", 0)
            if start_idx >= total_count: start_idx = 0
            end_idx = min(start_idx + batch_size, total_count)
            
            current_batch = links[start_idx:end_idx]

            logger.info(f"üìä Engine Stats: Total Links={total_count}, Current Batch={len(current_batch)}")
            logger.info(f"üéØ Cycle Target: Complete update every {CYCLE_HOURS} hours")
            
            ip_cache = {}
            results = []
            dead_links = set()
            
            async with aiohttp.ClientSession() as session:
                tasks = []
                for link in current_batch:
                    h, p = self.get_host_port(link)
                    tasks.append(self.check_node(session, h, p, ip_cache))
                
                checked_data = await asyncio.gather(*tasks)
                
                for idx, (ip, ping) in enumerate(checked_data):
                    link = current_batch[idx]
                    country = self.get_country(ip) if ip else None
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ª–∏–º–∏—Ç –ø–∏–Ω–≥–∞ –¥–ª—è —Å—Ç—Ä–∞–Ω—ã (–∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç 200)
                    limit = PING_LIMITS.get(country, PING_LIMITS['DEFAULT'])
                    
                    if ip and ping <= limit:
                        results.append({
                            "link": self.wrap_for_russia(link),
                            "country": country,
                            "ping": ping,
                            "score": 1000 if country in PRIORITY_REGIONS else 0
                        })
                    else:
                        # –ï—Å–ª–∏ –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –∏–ª–∏ –ø–∏–Ω–≥ > –ª–∏–º–∏—Ç–∞ ‚Äî –≤ –±–∞–Ω
                        dead_links.add(link)

            # 4. –ê—Ç–æ–º–∞—Ä–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º
            for filename in set(COUNTRY_MAP.values()) | {DEFAULT_MIX}:
                current_nodes = {}
                # –ß–∏—Ç–∞–µ–º —Å—Ç–∞—Ä—ã–µ, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –º–µ—Ä—Ç–≤—ã
                if os.path.exists(filename):
                    with open(filename, 'r', encoding='utf-8', errors='ignore') as f:
                        for l in f:
                            node = l.strip()
                            if node and node not in dead_links: current_nodes[node] = True
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∂–∏–≤—ã–µ
                for res in results:
                    if COUNTRY_MAP.get(res['country'], DEFAULT_MIX) == filename:
                        current_nodes[res['link']] = True
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ø-500
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(list(current_nodes.keys())[:MAX_NODES_PER_COUNTRY]) + '\n')

            # 5. –ì–ª–æ–±–∞–ª—å–Ω–∞—è —á–∏—Å—Ç–∫–∞ –º–∞—Å—Ç–µ—Ä-–±–∞–∑—ã (all_sources.txt)
            # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –º–µ—Ä—Ç–≤—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞–≤—Å–µ–≥–¥–∞
            remaining_master = [l for l in links if l not in dead_links]
            with open(SOURCE_FILE, 'w', encoding='utf-8') as f:
                f.write('\n'.join(remaining_master) + '\n')

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            self.state["last_index"] = end_idx if end_idx < total_count else 0
            self.state["processed_total"] += len(current_batch)
            self.state["last_run_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            self.save_state()
            
            logger.info(f"‚úÖ Batch Completed. Live: {len(results)}, Removed: {len(dead_links)}")
            logger.info(f"üìç Next check will start from index: {self.state['last_index']}")

        except Exception as e:
            logger.critical(f"FATAL ERROR: {e}", exc_info=True)
        finally:
            if os.path.exists(LOCK_FILE):
                os.remove(LOCK_FILE)

if __name__ == "__main__":
    parser = MonsterParser()
    asyncio.run(parser.run())
