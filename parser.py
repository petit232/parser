import os
import json
import asyncio
import aiohttp
import time
import re
import socket
import geoip2.database
import logging
import base64
import subprocess
import shutil
from datetime import datetime
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

# ==============================================================================
# --- CONFIGURATION & LOGGING SETUP ---
# ==============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("MonsterEngine")

# --- DIRECTORY STRUCTURE ---
DB_DIR = 'database'   
OUTPUT_DIR = 'proxy'  

# Files location
SOURCE_FILE = 'all_sources.txt' 
STATE_FILE = os.path.join(DB_DIR, 'monster_state.json')
GEOIP_DB = os.path.join(DB_DIR, 'GeoLite2-Country.mmdb')
LOCK_FILE = os.path.join(DB_DIR, '.monster.lock')

# Files inside "proxy"
LINKS_INFO_FILE = os.path.join(OUTPUT_DIR, 'LINKS_FOR_CLIENTS.txt')

# --- ENGINE CONSTANTS (STABLE PERFORMANCE MODE) ---
TIMEOUT = 5              # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 5—Å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö
MAX_CONCURRENCY = 800    # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è GitHub Actions, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –¥—Ä–æ–ø–æ–≤
BATCH_SIZE = 150000      # –õ–∏–º–∏—Ç –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥
LOG_STEP = 1000          # –ö–∞–∂–¥—ã–µ 1000 –Ω–æ–¥ –≤—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç—É—Å –≤ –∫–æ–Ω—Å–æ–ª—å

# --- NETWORK THRESHOLDS (MS) ---
PING_LIMITS = {
    'DEFAULT': 300,
    'US': 450, 
    'HK': 400, 
    'SG': 400, 
    'JP': 400,
    'BY': 250, 
    'KZ': 250, 
    'RU': 300
}

# –ü–æ—Ä–æ–≥–∏ –¥–ª—è "–∏–Ω–≤–∞–ª–∏–¥–Ω—ã—Ö" —Å–µ—Ä–≤–µ—Ä–æ–≤
INVALID_THRESHOLD_MAX = 600 
INVALID_REGIONS = {'BY', 'KZ', 'US', 'RU'}

# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
PRIORITY_REGIONS = {'BY', 'KZ', 'DE', 'FI', 'SE', 'LV', 'RU', 'US', 'CH', 'FR'}

# --- COUNTRY TO FILE MAPPING ---
COUNTRY_MAP = {
    'RU': 'russia.txt', 
    'BY': 'belarus.txt', 
    'DE': 'germany.txt',
    'FR': 'france.txt',
    'FI': 'finland.txt',
    'KZ': 'kazakhstan.txt',
    'LV': 'latvia.txt',
    'CH': 'switzerland.txt',
    'US': 'usa.txt',
    'SE': 'sweden.txt'
}
DEFAULT_MIX = 'mix.txt'
INVALID_FILE = 'invalid.txt' 
MAX_NODES_PER_FILE = 5000 # –ú–∞–∫—Å–∏–º—É–º –Ω–æ–¥ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª

class MonsterParser:
    def __init__(self):
        self.start_time = time.time()
        self.ensure_structure()
        self.clean_cyrillic_files()
        self.migrate_old_data()
        self.state = self.load_state()
        self.geo_reader = self.init_geo()
        
        # –†–µ–≥—É–ª—è—Ä–∫–∏ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ —Å—Å—ã–ª–æ–∫
        self.proxy_pattern = re.compile(r'(vless|vmess|trojan|ss|ssr)://[^\s"\'<>()]+')
        self.ip_pattern = re.compile(r'@?([\w\.-]+):(\d+)')
        
        self.semaphore = asyncio.Semaphore(MAX_CONCURRENCY)
        self.mandatory_files = set(COUNTRY_MAP.values()) | {DEFAULT_MIX, INVALID_FILE}
        
        # –°—á–µ—Ç—á–∏–∫–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –ª–æ–≥–æ–≤
        self.counter_checked = 0
        self.counter_alive = 0
        self.counter_dead = 0

    def ensure_structure(self):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–ø–∫–∏ –∏ —Ñ–∞–π–ª—ã –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç."""
        for d in [DB_DIR, OUTPUT_DIR]:
            if not os.path.exists(d):
                os.makedirs(d, exist_ok=True)
        if not os.path.exists(SOURCE_FILE):
            with open(SOURCE_FILE, 'w', encoding='utf-8') as f:
                f.write("# Monster Engine Source List\n")

    def clean_cyrillic_files(self):
        """–£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª—ã —Å —Ä—É—Å—Å–∫–∏–º–∏ –∏–º–µ–Ω–∞–º–∏ –∏–∑ –ø–∞–ø–∫–∏ proxy."""
        if os.path.exists(OUTPUT_DIR):
            for file in os.listdir(OUTPUT_DIR):
                if re.search(r'[–ê-–Ø–∞-—è]', file):
                    try: os.remove(os.path.join(OUTPUT_DIR, file))
                    except: pass

    def migrate_old_data(self):
        """–ü–µ—Ä–µ–Ω–æ—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—Ç–∞—Ä—ã—Ö –ø–∞–ø–æ–∫ –µ—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç."""
        old_folders = {'–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö': DB_DIR, '–ø—Ä–æ–∫—Å–∏': OUTPUT_DIR}
        for old, new in old_folders.items():
            if os.path.exists(old) and os.path.isdir(old):
                for item in os.listdir(old):
                    src = os.path.join(old, item)
                    dst = os.path.join(new, item)
                    try:
                        if os.path.exists(dst): os.remove(src)
                        else: shutil.move(src, dst)
                    except: pass
                try: os.rmdir(old)
                except: pass

    def init_geo(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GeoIP –±–∞–∑—ã."""
        if not os.path.exists(GEOIP_DB): return None
        try:
            return geoip2.database.Reader(GEOIP_DB)
        except: return None

    def load_state(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞."""
        default = {"processed_total": 0}
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE, 'r', encoding='utf-8') as f:
                    return {**default, **json.load(f)}
            except: pass
        return default

    def save_state(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ."""
        try:
            with open(STATE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=4, ensure_ascii=False)
        except: pass

    def get_host_port(self, link):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–æ—Å—Ç –∏ –ø–æ—Ä—Ç –∏–∑ —Å—Å—ã–ª–∫–∏."""
        try:
            match = self.ip_pattern.search(link)
            if match: return match.group(1), match.group(2)
        except: pass
        return None, None

    async def fetch_subscription(self, session, url):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –ø–æ–¥–ø–∏—Å–∫—É –∏ –ø–∞—Ä—Å–∏—Ç —Å—Å—ã–ª–∫–∏."""
        try:
            async with session.get(url, timeout=25) as response:
                if response.status == 200:
                    text = await response.text()
                    try:
                        decoded = base64.b64decode(text.strip()).decode('utf-8', errors='ignore')
                        if '://' in decoded: text = decoded
                    except: pass
                    return [m.group(0) for m in self.proxy_pattern.finditer(text)]
        except: pass
        return []

    async def check_node(self, session, host, port, ip_cache):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–¥—ã: DNS —Ä–µ–∑–æ–ª–≤ + TCP –∫–æ–Ω–Ω–µ–∫—Ç —Å –¥–≤–æ–π–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π."""
        if not host or not port: 
            self.counter_checked += 1
            self.counter_dead += 1
            return None, 9999
            
        key = f"{host}:{port}"
        if key in ip_cache: 
            res = ip_cache[key]
            self.counter_checked += 1
            if res[0]: self.counter_alive += 1
            else: self.counter_dead += 1
            return res
        
        async with self.semaphore:
            # –°–∏—Å—Ç–µ–º–∞ –¥–≤–æ–π–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ (Double Check)
            for attempt in range(2):
                start = time.time()
                try:
                    # DNS Resolve
                    ip = await asyncio.wait_for(
                        asyncio.get_event_loop().run_in_executor(None, socket.gethostbyname, host),
                        timeout=TIMEOUT
                    )
                    # TCP Connect
                    _, writer = await asyncio.wait_for(
                        asyncio.open_connection(ip, int(port)),
                        timeout=TIMEOUT
                    )
                    writer.close()
                    await writer.wait_closed()
                    
                    latency = int((time.time() - start) * 1000)
                    res = (ip, latency)
                    ip_cache[key] = res
                    
                    self.counter_checked += 1
                    self.counter_alive += 1
                    
                    if self.counter_checked % LOG_STEP == 0:
                        logger.info(f"‚ö° –ü—Ä–æ–≥—Ä–µ—Å—Å: {self.counter_checked} –Ω–æ–¥ | –ñ–∏–≤—ã—Ö: {self.counter_alive} | –ú–µ—Ä—Ç–≤—ã—Ö: {self.counter_dead}")
                    
                    return res
                except:
                    if attempt == 0: 
                        await asyncio.sleep(0.5) # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –≤—Ç–æ—Ä–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                        continue
                    
                    ip_cache[key] = (None, 9999)
                    self.counter_checked += 1
                    self.counter_dead += 1
                    if self.counter_checked % LOG_STEP == 0:
                        logger.info(f"‚ö° –ü—Ä–æ–≥—Ä–µ—Å—Å: {self.counter_checked} –Ω–æ–¥ | –ñ–∏–≤—ã—Ö: {self.counter_alive} | –ú–µ—Ä—Ç–≤—ã—Ö: {self.counter_dead}")
                    return None, 9999

    def get_country_code(self, ip):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–∞–Ω—É –ø–æ IP."""
        if not self.geo_reader or not ip: return None
        try: return self.geo_reader.country(ip).country.iso_code
        except: return None

    def apply_fragmentation(self, link):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ –≤ –†–§."""
        try:
            parsed = urlparse(link)
            if parsed.scheme in ['vless', 'vmess', 'trojan']:
                query = parse_qs(parsed.query)
                if 'reality' in str(query).lower(): return link
                query['fragment'] = ['10-20,30-50']
                query['mux'] = ['enable=true&concurrency=8']
                parts = list(parsed)
                parts[4] = urlencode(query, doseq=True)
                return urlunparse(parts)
        except: pass
        return link

    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã –¥–≤–∏–∂–∫–∞."""
        if os.path.exists(LOCK_FILE):
            if (time.time() - os.path.getmtime(LOCK_FILE)) < 900:
                logger.warning("–ü—Ä–æ—Ü–µ—Å—Å —É–∂–µ –∑–∞–ø—É—â–µ–Ω. –ü—Ä–æ–ø—É—Å–∫.")
                return
        
        try:
            with open(LOCK_FILE, 'w') as f: f.write(str(time.time()))
            
            # 1. –ß—Ç–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            with open(SOURCE_FILE, 'r', encoding='utf-8', errors='ignore') as f:
                raw = [l.strip() for l in f if len(l.strip()) > 10 or l.startswith('#')]
            
            subs = [u for u in raw if u.startswith('http') and not u.startswith('#')]
            configs = [c for c in raw if '://' in c and not c.startswith('http') and not c.startswith('#')]
            
            logger.info(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(subs)} –ø–æ–¥–ø–∏—Å–æ–∫, {len(configs)} —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ñ–∏–≥–æ–≤")

            all_links = configs.copy()
            async with aiohttp.ClientSession() as session:
                sub_data = await asyncio.gather(*[self.fetch_subscription(session, u) for u in subs])
                for links in sub_data: all_links.extend(links)
                
                unique_pool = list(set(all_links))
                total = len(unique_pool)
                logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–¥: {total}")
                
                if total == 0: 
                    logger.warning("–°–ø–∏—Å–æ–∫ –ø—É—Å—Ç. –í—ã—Ö–æ–¥.")
                    return

                batch = unique_pool[:BATCH_SIZE]
                logger.info(f"‚ö° –ù–∞—á–∏–Ω–∞—é –ø—Ä–æ–≤–µ—Ä–∫—É {len(batch)} –Ω–æ–¥ (–ü–æ—Ç–æ–∫–æ–≤: {MAX_CONCURRENCY})...")
                
                ip_cache = {}
                tasks = [self.check_node(session, *self.get_host_port(l), ip_cache) for l in batch]
                results = await asyncio.gather(*tasks)
                
                valid_nodes = []
                dead_set = set()
                
                for i, (ip, ping) in enumerate(results):
                    link = batch[i]
                    if not ip:
                        dead_set.add(link)
                        continue
                        
                    cc = self.get_country_code(ip)
                    limit = PING_LIMITS.get(cc, PING_LIMITS['DEFAULT'])
                    
                    if ping <= limit:
                        final_link = self.apply_fragmentation(link) if cc == 'RU' else link
                        valid_nodes.append({"link": final_link, "cc": cc, "type": "good", "ping": ping})
                    elif cc in INVALID_REGIONS and ping <= INVALID_THRESHOLD_MAX:
                        valid_nodes.append({"link": link, "cc": cc, "type": "invalid", "ping": ping})
                    else:
                        dead_set.add(link)

            # --- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º ---
            file_stats = {f: 0 for f in self.mandatory_files}
            for filename in self.mandatory_files:
                target_path = os.path.join(OUTPUT_DIR, filename)
                current_nodes = []
                
                for node in valid_nodes:
                    node_cc = node['cc']
                    if node['type'] == "invalid" and filename == INVALID_FILE:
                        current_nodes.append(node['link'])
                    elif node['type'] == "good":
                        if COUNTRY_MAP.get(node_cc, DEFAULT_MIX) == filename:
                            current_nodes.append(node['link'])

                final_list = list(set(current_nodes))[:MAX_NODES_PER_FILE]
                file_stats[filename] = len(final_list)

                with open(target_path, 'w', encoding='utf-8') as f:
                    if final_list: f.write('\n'.join(final_list) + '\n')
                    else: f.write(f"# Monster Engine: Offline {datetime.now().strftime('%H:%M:%S')}\n")

            # –ó–∞—â–∏—Ç–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–Ω–µ —É–¥–∞–ª—è–µ–º —Ç–æ, —á—Ç–æ –∂–∏–≤–æ –∏–ª–∏ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–¥–ø–∏—Å–∫–æ–π)
            updated_sources = []
            for s in raw:
                if s.startswith('http') or s.startswith('#'): 
                    updated_sources.append(s)
                elif s in configs and s not in dead_set: 
                    updated_sources.append(s)
            
            with open(SOURCE_FILE, 'w', encoding='utf-8') as f:
                f.write('\n'.join(updated_sources) + '\n')

            # –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—à–±–æ—Ä–¥ –≤ –ª–æ–≥–∏
            logger.info("="*50)
            logger.info(f"üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ | –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: {len(batch)} | –ñ–∏–≤—ã—Ö: {len(valid_nodes)} | –ú–µ—Ä—Ç–≤—ã—Ö: {len(dead_set)}")
            logger.info("-" * 50)
            for f, c in sorted(file_stats.items(), key=lambda x: x[1], reverse=True):
                icon = "üü¢" if c > 0 else "üî¥"
                logger.info(f"   {icon} {f.ljust(18)} : {c} –Ω–æ–¥")
            logger.info("="*50)

            self.state["processed_total"] += len(batch)
            self.save_state()

        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        finally:
            if os.path.exists(LOCK_FILE): 
                try: os.remove(LOCK_FILE)
                except: pass

if __name__ == "__main__":
    asyncio.run(MonsterParser().run())
