import os
import json
import asyncio
import aiohttp
import time
import re
import socket
import geoip2.database
import logging
import base64
import subprocess
import shutil
from datetime import datetime
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

# ==============================================================================
# --- CONFIGURATION & LOGGING SETUP ---
# ==============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("MonsterEngine")

# --- DIRECTORY STRUCTURE ---
DB_DIR = 'database'   # –°–∏—Å—Ç–µ–º–Ω–∞—è –ø–∞–ø–∫–∞
OUTPUT_DIR = 'proxy'  # –ü–∞–ø–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (–ø—É–±–ª–∏—á–Ω–∞—è)

# Files inside "database"
SOURCE_FILE = 'all_sources.txt'
STATE_FILE = os.path.join(DB_DIR, 'monster_state.json')
GEOIP_DB = os.path.join(DB_DIR, 'GeoLite2-Country.mmdb')
LOCK_FILE = os.path.join(DB_DIR, '.monster.lock')

# Files inside "proxy"
LINKS_INFO_FILE = os.path.join(OUTPUT_DIR, 'LINKS_FOR_CLIENTS.txt')

# --- ENGINE CONSTANTS ---
TIMEOUT = 3              # –¢–∞–π–º–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
MAX_CONCURRENCY = 150    # –ö–æ–ª-–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
CYCLE_HOURS = 3          # –í—Ä–µ–º—è –ø–æ–ª–Ω–æ–≥–æ –∫—Ä—É–≥–∞ –æ–±—Ö–æ–¥–∞ –±–∞–∑—ã
BATCH_INTERVAL_MIN = 20  # –ò–Ω—Ç–µ—Ä–≤–∞–ª –∑–∞–ø—É—Å–∫–∞ (–º–∏–Ω—É—Ç—ã)

# --- NETWORK THRESHOLDS ---
PING_LIMITS = {
    'DEFAULT': 250,
    'US': 300, 
    'HK': 300, 
    'SG': 300, 
    'JP': 300,
    'BY': 200, 
    'KZ': 200, 
    'RU': 250
}

# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
PRIORITY_REGIONS = {'BY', 'KZ', 'DE', 'FI', 'SE', 'LV', 'RU', 'US', 'CH', 'FR'}

# --- COUNTRY TO FILE MAPPING ---
# –¢–æ–ª—å–∫–æ —ç—Ç–∏ —Å—Ç—Ä–∞–Ω—ã –ø–æ–ª—É—á–∞—é—Ç —Å–≤–æ–∏ —Ñ–∞–π–ª—ã. –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∏–¥—É—Ç –≤ mix.txt.
COUNTRY_MAP = {
    'RU': 'russia.txt', 
    'BY': 'belarus.txt', 
    'DE': 'germany.txt',
    'FR': 'france.txt',
    'FI': 'finland.txt',
    'KZ': 'kazakhstan.txt',
    'LV': 'latvia.txt',
    'CH': 'switzerland.txt',
    'US': 'usa.txt',
    'SE': 'sweden.txt'
}
DEFAULT_MIX = 'mix.txt'
MAX_NODES_PER_FILE = 500

class MonsterParser:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞, –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ–∫—Å–∏-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π.
    """
    
    def __init__(self):
        self.start_time = time.time()
        self.ensure_structure()
        self.migrate_old_data()
        self.state = self.load_state()
        self.geo_reader = self.init_geo()
        
        # –†–µ–≥—É–ª—è—Ä–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.proxy_pattern = re.compile(r'(vless|vmess|trojan|ss|ssr)://[^\s"\'<>()]+')
        self.ip_pattern = re.compile(r'@?([\w\.-]+):(\d+)')
        
        self.semaphore = asyncio.Semaphore(MAX_CONCURRENCY)
        self.active_files = set(COUNTRY_MAP.values()) | {DEFAULT_MIX}

    def ensure_structure(self):
        """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫."""
        os.makedirs(DB_DIR, exist_ok=True)
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        if not os.path.exists(SOURCE_FILE):
            with open(SOURCE_FILE, 'w', encoding='utf-8') as f:
                f.write("# –í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–¥–ø–∏—Å–∫–∏ –∏–ª–∏ –∫–æ–Ω—Ñ–∏–≥–∏\n")

    def migrate_old_data(self):
        """–ü–µ—Ä–µ–Ω–æ—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—Ç–∞—Ä—ã—Ö –ø–∞–ø–æ–∫ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –≤ –Ω–æ–≤—ã–µ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ."""
        old_folders = {'–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö': DB_DIR, '–ø—Ä–æ–∫—Å–∏': OUTPUT_DIR}
        for old, new in old_folders.items():
            if os.path.exists(old) and os.path.isdir(old):
                logger.info(f"üîÑ –ú–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ '{old}' –≤ '{new}'...")
                for item in os.listdir(old):
                    s = os.path.join(old, item)
                    d = os.path.join(new, item)
                    try:
                        if os.path.exists(d): os.remove(s)
                        else: shutil.move(s, d)
                    except: pass
                try: os.rmdir(old)
                except: pass

    def init_geo(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã GeoIP."""
        if os.path.exists(GEOIP_DB):
            try: return geoip2.database.Reader(GEOIP_DB)
            except: pass
        return None

    def load_state(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–≤–∏–∂–∫–∞."""
        default_state = {"last_index": 0, "processed_total": 0, "dead_total": 0}
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE, 'r', encoding='utf-8') as f:
                    return {**default_state, **json.load(f)}
            except: pass
        return default_state

    def save_state(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è."""
        try:
            with open(STATE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=4, ensure_ascii=False)
        except: pass

    def get_host_port(self, link):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–æ—Å—Ç–∞ –∏ –ø–æ—Ä—Ç–∞ –∏–∑ —Å—Å—ã–ª–∫–∏."""
        try:
            match = self.ip_pattern.search(link)
            if match: return match.group(1), match.group(2)
        except: pass
        return None, None

    async def fetch_subscription(self, session, url):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–ø–∏—Å–∫–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫."""
        try:
            async with session.get(url, timeout=15) as response:
                if response.status == 200:
                    raw = await response.text()
                    try:
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ base64
                        decoded = base64.b64decode(raw.strip()).decode('utf-8', errors='ignore')
                        raw = decoded if '://' in decoded else raw
                    except: pass
                    return [m.group(0) for m in self.proxy_pattern.finditer(raw)]
        except Exception as e:
            logger.debug(f"Fetch error {url}: {e}")
        return []

    async def check_node(self, session, host, port, ip_cache):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —É–∑–ª–∞."""
        if not host or not port: return None, 9999
        cache_key = f"{host}:{port}"
        if cache_key in ip_cache: return ip_cache[cache_key]
        
        async with self.semaphore:
            start = time.time()
            try:
                # –†–µ–∑–æ–ª–≤ DNS
                ip = await asyncio.wait_for(
                    asyncio.get_event_loop().run_in_executor(None, socket.gethostbyname, host), 
                    timeout=TIMEOUT
                )
                # –ü–æ–ø—ã—Ç–∫–∞ TCP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                _, writer = await asyncio.wait_for(
                    asyncio.open_connection(ip, int(port)), 
                    timeout=TIMEOUT
                )
                writer.close()
                await writer.wait_closed()
                
                ping = int((time.time() - start) * 1000)
                ip_cache[cache_key] = (ip, ping)
                return ip, ping
            except:
                ip_cache[cache_key] = (None, 9999)
                return None, 9999

    def get_country(self, ip):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –ø–æ IP."""
        if not self.geo_reader or not ip: return None
        try: return self.geo_reader.country(ip).country.iso_code
        except: return None

    def wrap_for_russia(self, link):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥–∞ –¥–ª—è –†–§ (fragment/mux)."""
        try:
            parsed = urlparse(link)
            if parsed.scheme in ['vless', 'vmess', 'trojan']:
                query = parse_qs(parsed.query)
                if 'reality' in str(query).lower(): return link
                query['fragment'] = ['10-20,30-50']
                query['mux'] = ['enable=true&concurrency=8']
                new_parts = list(parsed)
                new_parts[4] = urlencode(query, doseq=True)
                return urlunparse(new_parts)
        except: pass
        return link

    def cleanup_obsolete_files(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö .txt —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –º–∞–ø–µ."""
        try:
            allowed = self.active_files | {os.path.basename(LINKS_INFO_FILE)}
            for f in os.listdir(OUTPUT_DIR):
                if f.endswith('.txt') and f not in allowed:
                    os.remove(os.path.join(OUTPUT_DIR, f))
        except: pass

    def update_links_for_clients(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ñ–∞–π–ª–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º —Å—Å—ã–ª–æ–∫ (–±–µ–∑ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã)."""
        try:
            repo_url = ""
            try:
                remote = subprocess.check_output(["git", "config", "--get", "remote.origin.url"]).decode().strip()
                path = remote.replace("git@github.com:", "").replace("https://github.com/", "").replace(".git", "")
                repo_url = f"https://raw.githubusercontent.com/{path}/main/{OUTPUT_DIR}"
            except:
                repo_url = f"https://raw.githubusercontent.com/USER/REPO/main/{OUTPUT_DIR}"

            content = [
                "üöÄ MONSTER ENGINE SUBSCRIPTIONS",
                "-"*40,
                f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                ""
            ]
            
            for filename in sorted(list(self.active_files)):
                display_name = filename.replace('.txt','').upper()
                content.append(f"üìç {display_name}:")
                content.append(f"{repo_url}/{filename}")
                content.append("")
            
            with open(LINKS_INFO_FILE, 'w', encoding='utf-8') as f:
                f.write('\n'.join(content))
            logger.info("üìÑ LINKS_FOR_CLIENTS.txt updated.")
        except Exception as e:
            logger.error(f"Links file update error: {e}")

    async def run(self):
        """–ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã."""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        if os.path.exists(LOCK_FILE) and (time.time() - os.path.getmtime(LOCK_FILE)) < 1200:
            return
            
        try:
            with open(LOCK_FILE, 'w') as f: f.write(str(time.time()))
            if not os.path.exists(SOURCE_FILE): return

            # –ß—Ç–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            with open(SOURCE_FILE, 'r', encoding='utf-8', errors='ignore') as f:
                raw_entries = list(dict.fromkeys([l.strip() for l in f if len(l.strip()) > 5 and not l.startswith('#')]))
            
            subscriptions = [e for e in raw_entries if e.startswith('http')]
            direct_configs = [e for e in raw_entries if '://' in e and not e.startswith('http')]

            all_links = direct_configs.copy()
            
            async with aiohttp.ClientSession() as session:
                # –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∏–∑ –ø–æ–¥–ø–∏—Å–æ–∫
                sub_results = await asyncio.gather(*[self.fetch_subscription(session, url) for url in subscriptions])
                for res in sub_results: all_links.extend(res)

                total_pool = set(all_links)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ-—Ñ–∞–π–ª
                self.update_links_for_clients()
                
                # –ï—Å–ª–∏ –Ω–æ–¥ –Ω–µ—Ç, "—Ç—Ä–æ–≥–∞–µ–º" —Ñ–∞–π–ª—ã –∏ –≤—ã—Ö–æ–¥–∏–º
                if not total_pool:
                    for filename in self.active_files:
                        path = os.path.join(OUTPUT_DIR, filename)
                        with open(path, 'w') as f: f.write('')
                        os.utime(path, None)
                    return

                # –†–∞—Å—á–µ—Ç –±–∞—Ç—á–∞
                batch_size = max(500, int(len(total_pool) / ((CYCLE_HOURS * 60) / BATCH_INTERVAL_MIN)))
                sorted_pool = list(total_pool)
                sorted_pool.sort(key=lambda x: any(p in x.upper() for p in PRIORITY_REGIONS), reverse=True)
                
                start_idx = self.state.get("last_index", 0)
                if start_idx >= len(sorted_pool): start_idx = 0
                current_batch = sorted_pool[start_idx : start_idx + batch_size]
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞—Ç—á–∞
                ip_cache = {}
                tasks = [self.check_node(session, *self.get_host_port(link), ip_cache) for link in current_batch]
                checked = await asyncio.gather(*tasks)
                
                live_results = []
                dead_links = set()
                
                for idx, (ip, ping) in enumerate(checked):
                    link = current_batch[idx]
                    country = self.get_country(ip)
                    limit = PING_LIMITS.get(country, PING_LIMITS['DEFAULT'])
                    
                    if ip and ping <= limit:
                        node_link = self.wrap_for_russia(link) if country == 'RU' else link
                        live_results.append({"link": node_link, "country": country})
                    else:
                        dead_links.add(link)

            # –ß–∏—Å—Ç–∫–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
            self.cleanup_obsolete_files()
            
            for filename in self.active_files:
                path = os.path.join(OUTPUT_DIR, filename)
                nodes = {}
                
                # –ß–∏—Ç–∞–µ–º —Å—Ç–∞—Ä—ã–µ (–µ—Å–ª–∏ –æ–Ω–∏ –µ—â–µ –∂–∏–≤—ã)
                if os.path.exists(path):
                    with open(path, 'r', encoding='utf-8') as f:
                        for l in f:
                            n = l.strip()
                            if n in total_pool and n not in dead_links: nodes[n] = True
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ
                for res in live_results:
                    target = COUNTRY_MAP.get(res['country'], DEFAULT_MIX)
                    if target == filename:
                        nodes[res['link']] = True
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –¥–∞—Ç—É —Ñ–∞–π–ª–∞ (mtime)
                with open(path, 'w', encoding='utf-8') as f:
                    if nodes:
                        f.write('\n'.join(list(nodes.keys())[:MAX_NODES_PER_FILE]) + '\n')
                    else:
                        f.write('')
                os.utime(path, None) # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞—Ç—ã –¥–ª—è GitHub

            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è all_sources.txt
            final_sources = [e for e in raw_entries if e.startswith('http') or (e in total_pool and e not in dead_links)]
            with open(SOURCE_FILE, 'w', encoding='utf-8') as f:
                f.write('\n'.join(final_sources) + '\n')

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            self.state.update({
                "last_index": start_idx + batch_size, 
                "last_run": datetime.now().isoformat(),
                "processed_total": self.state.get("processed_total", 0) + len(current_batch)
            })
            self.save_state()
            logger.info("‚úÖ Cycle finished successfully.")

        except Exception as e:
            logger.error(f"üí• Critical error in run: {e}", exc_info=True)
        finally:
            if os.path.exists(LOCK_FILE):
                try: os.remove(LOCK_FILE)
                except: pass

if __name__ == "__main__":
    asyncio.run(MonsterParser().run())
